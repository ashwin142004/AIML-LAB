Architecture
-input layer
- Hidden, intermediate, or invisible layer
- Output layer

how layers are interconnected:
- single layer feedforward networks
- multi layer feedforward networks
- recurrent or feedback Architecture
- Mesh Architectures

Perceptron
Single Layered network
representational power of perceptrons 
linearly seperable 


Gradient Descent and the delta rule 
https://vtupulse.com/machine-learning/gradient-descent-and-delta-rule/

Features of gradient descent
GD is an important general paradigm for learning. It is a strategy for searching through a large or infinite hypothesis space that can be applied whenever: 
    the hypothesis space contains continuously parameterised hypothesis
    the error can be differentiated with respect to these hypothesis parameters 

the key practical difficulties in applying gradient descent are:
    converging to a local minimum can sometimes be quite slow
    if there are multiple local minima in the error surface, then there is no guarentee that the procedure will find the global minimum.

Naive Bayesian Classifier

Condtional independence 

A deisise affects one in thousande poeple. A test detects the deisess correctly 99% of the time and incorrectly gives a positive result in 5% of healthy people. 
what is the probability that the person actually has the dieseas if they test positive. 

Given a disease affecting 1 in 1000 people, a test with 99% accuracy, and a 5% false positive rate, 
the probability of having the disease given a positive test result is approximately 16.5%. 

Here's a breakdown using Bayes' Theorem: 
P(Disease) = 1/1000 = 0.001: (Prevalence of the disease)
P(Test Positive | Disease) = 0.99: (Test accuracy)
P(Test Positive | No Disease) = 0.05: (False positive rate)

Applying Bayes' Theorem:
P(Disease | Test Positive) = [P(Test Positive | Disease) * P(Disease)] / [P(Test Positive | Disease) * P(Disease) + P(Test Positive | No Disease) * P(No Disease)] 
P(Disease | Test Positive) = [0.99 * 0.001] / [0.99 * 0.001 + 0.05 * 0.999]
P(Disease | Test Positive) = 0.00099 / (0.00099 + 0.04995)
P(Disease | Test Positive) = 0.00099 / 0.05094
P(Disease | Test Positive) ≈ 0.0194 or 1.94%

Therefore, the probability of having the disease given a positive test result is approximately 1.94% 

A student has a 70% chance of being good at math, if they are good at math they can solve a tough problem 80% of the time. 
If they are not only 20% of the time they can solve. If a student solves a tough problem, what is the probabilitythat they are good at math.

P(G) = 0.7
p(S|G) = 0.8

p(!G) = 0.3
p(S|!G) = 0.2

p(S) = 0.8*0.7 + 0.2*0.3
p(S) = 0.62

p(G|S) = 0.903


person  affecting   income  buy computer?
1       young       high        No
2       young       low         yes
3       middle      med         yes
4       old         high        yes
5       old         med         no

age = youung
income = med

Total: 5 people

P(Yes) = 3/5

P(No) = 2/5

We’re interested in:

P(age=young | buys=yes)

P(income=med | buys=yes)

For class Yes (3 examples):
From persons 2, 3, and 4

age=young: 1/3

income=med: 1/3

For class No (2 examples):
From persons 1 and 5

age=young: 1/2

income=med: 1/2

P(Yes | age=young, income=med)
= P(age=young | Yes) × P(income=med | Yes) × P(Yes)
= (1/3) × (1/3) × (3/5)
= 1/15

P(No | age=young, income=med)
= P(age=young | No) × P(income=med | No) × P(No)
= (1/2) × (1/2) × (2/5)
= 1/10

To find the actual probability, normalize the values:

Total = (1/15 + 1/10) = (2/30 + 3/30) = 5/30

Now:

P(Yes | conditions) = (1/15) ÷ (5/30) = (2/10) = 0.2

P(No | conditions) = (1/10) ÷ (5/30) = (3/10) = 0.3

Since P(No | age=young, income=med) > P(Yes | ...), the model predicts: No, the person is not likely to buy a computer.